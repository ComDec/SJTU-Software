# SVM实操

519111910306
姜戈

## 1.介绍
一个分类任务通常包括将数据分离成**训练集**和**测试集**。
训练集中的每个实例包含一个“目标值”(即类标签)和几个“属性”(即**特征或观察到的变量**)。SVM的目标是生成一个模型(基于训练数据)，该模型仅给出测试数据的属性来预测测试数据的目标值。

给定一个实例标签的训练集$(x_i,y_i),i=1,\dots,l; x_i\in R^n,y\in \{1,-1\}$

优化问题的形式![image-20210514192254114](C:\Users\chell\AppData\Roaming\Typora\typora-user-images\image-20210514192254114.png)

在这里，训练向量$x_i$被映射到一个更高维的空间利用函数$φ$。支持向量机找到一个边界最大的线性分离超平面

在这个高维空间中。C是误差项的惩罚参数

$K(x_i,x_j)≡φ(xi)^T φ(xj)$称为核函数。

以下是四个基本的核函数：

+ 线性：$K(X_i,X_j)=x_i ^Tx_j$
+ 多项式：$K(X_i,X_j)=(\gamma x_i ^Tx_j+r)^d,\gamma >0$
+ 径向基:$K(x_i , x_j ) = exp(−γ||x_i − x_j||^2 ), γ > 0.$
+ sigmoid:$K(x_i , x_j ) = tanh(γx_i^ T x_j + r)$

$γ、r、d$是核参数。

### 1.1实际例子

### 1.2过程设计

+ 将数据转换为SVM包的格式
+ 对数据进行简单的缩放
+ 考虑RBF核$K(x,y) = e^{−\gamma ||x−y||^2}$
+ 使用交叉验证找到最佳参数$C$和$γ$
+ 使用最佳参数$C$和$γ$训练整个训练集
+ 测试

接下来将在下面的部分中详细讨论这个过程

## 2.数据处理

### 2.1范畴特征

SVM要求每个数据实例被表示为一个实数的向量。
因此，如果有分类属性，我们首先要把它们转换成数字属性数据。
可以使用m个数字来表示m类属性。
m个数字中只有一个是1，其他都是0。

例如，一个三类的属性可以表示为(0,0,1), (0,1,0), 和(1,0,0)。
经验表明，如果一个属性的值的数量不是太大。这种编码方式可能比使用单个数字更稳定。

最佳参数可能会受到数据集大小的影响，但在实践中，从交叉验证中得到的参数已经适合整个训练集。但在实践中，从交叉验证中得到的参数已经适用于整个训练集。

### 2.2缩放

在应用SVM之前进行缩放是非常重要的。缩放的主要优点是避免大数值范围内的属性支配小数值范围内的属性。另一个优点是可以避免计算过程中的数字困难。
因为核值通常取决于特征向量的内积，例如线性核和多项式核，大的属性值可能会导致数值上的问题。 例如，线性核和多项式核，大的属性值可能会导致数字问题。例如：
> 建议线性缩放每个属性到[-1; +1]或[0; 1]的范围。

当然，也必须用同样的方法来扩展训练和测试数据。
例如，假设我们将训练数据的第一个属性从[-10; +10缩放]到[-1; +1]。如果测试数据的第一个属性位于[-11; +8]的范围内，则必须将测试数据缩放到[-1.1；+0.8]。**(附录B)**

## 3.模型选择

尽管在第1节中只提到了四个常见的内核，但必须决定首先尝试哪一个。然后选择惩罚参数C和内核参数$\gamma$的选择

### 3.1 RBF核

一般来说，**RBF核是一个合理的第一选择**。这种内核**非线性**地将样本映射到一个更高的维度空间，所以它与线性核不同，可以处理当类标签和属性之间的关系是非线性的情况。
此外。线性核是RBF的一个特例 ，因为线性核有一个惩罚参数C。
因为带有惩罚参数C的线性核与带有某些参数（C，γ）的RBF核具有相同的性能。
一些参数(C, γ)。此外，在某些参数下，sigmoid核的表现与RBF类似。

第二个原因是超参数的数量，它影响了模型选择的复杂性。多项式核比RBF核有更多的超参数。

最后，RBF核有较少的数字困难。一个关键点是$0 <K_{ij} ≤ 1$，与此相反，多项式核的核值可能达到无穷大。此外，必须注意，某些参数下，sigmoid核是无效的（也就是说，并不是两个向量的内积）。

### 3.2 交叉验证和网格搜索（重点）

RBF核有两个参数。$C和γ$。对于哪个C和γ对给定的问题是最好的是未知的；因此，必须进行某种模型选择（参数搜索）。我们的目标是确定好的（C, γ），以便分类器能够准确地预测未知的数据。

分类器能够准确预测未知数据（即测试数据）。需要注意的是实现高的训练精度可能没有用（即一个分类器能够准确地预测的训练数据的类标签确实是已知的）。

**所以一个常见的策略是将数据集分成两部分，其中一部分被认为是未知。从 "未知 "集得到的预测准确率更准确地反映了对独立数据集进行分类的性能。这个程序的一个改进版本该程序的改进版本被称为交叉验证。**

在v-fold交叉验证法中，我们首先将训练集分成v个大小相等的子集。依次用剩余的v-1个子集上训练的分类器测试一个子集。因此，整个训练集的每个实例都被预测了一次，所以交叉验证的准确性是指正确分类的数据的百分比。

交叉验证程序可以防止过拟合问题,见下图例子![](file:///home/egotist/SVM/figure1.jpg)
                一个过拟合的分类器和一个更好的分类器（填充的圆圈和
                  三角形是训练数据，而空心圆和三角形是测试数据。）

图1a和1b中的分类器的测试精度并不高，因为它过度拟合了训练数据。
如果我们把图1a和1b中的训练和测试数据看作是交叉验证中的训练和验证集，那么准确率就不高了。
另一方面，图1c和1d中的分类器没有过度拟合训练数据，并给出了较好的交叉验证结果。训练和测试的准确度也更好。

所以建议使用交叉验证法对$C和γ$进行 "网格搜索"。对各种$(C, γ)$值进行尝试，并挑选出交叉验证准确率最高的一个。
我们发现，**尝试指数级增长的$C和γ$序列是一种实用的方法，可以识别出好的参数**（例如，$C = 2^-5 , 2^-3 , ... ... , 2……15 ,
γ = 2^-15 , 2^-13 , . . . , 2^3 $).网格搜索思路简单但过于浪费性能。

事实上，有几种的先进方法可以节省计算成本，例如，通过近似的交叉验证率。

然而，有两个动机使我们更倾向于采用简单的网格搜索法:

1. 其一是在心理上，不放心使用那些避免了用近似值或启发式方法来避免详尽的参数搜索。

2. 另一个原因是，通过网格搜索找到好的参数所需的计算时间并不比先进的方法多多少，因为只有两个参数法。
   此外，网格搜索可以很容易地并行化，因为每个$(C, γ)$是独立的。许多先进的方法都是迭代过程，例如，沿着路径行走，可能很难并行化。

由于做一个完整的网格搜索可能仍然很费时，我们建议首先使用泛化的网格。在确定了网格上一个 "较好 "的区域后，就可以对该区域进行更细的网格搜索。为了说明这一点，我们做了一个实验

   我们首先使用一个粗略的网格，发现最佳的（C, γ）是$（2^3 , 2^{-5} ）$。交叉验证率为77.5%。![](file:///home/egotist/SVM/figure2.jpg)接下来，我们在$（2^3 , 2^{-5} ）$附近进行了更细的网格搜索。
   (2 3 , 2 -5 )附近进行更细的网格搜索（图3），得到更好的交叉验证率77.6%。
   在(2 3.25 , 2 -5.25 )获得更好的交叉验证率。![](file:///home/egotist/SVM/figure3.jpg)在找到最佳的(C, γ)之后，整个训练集再次被训练,来生成最终的分类器。



上述方法对有数千或更多数据点的问题很有效。
 **对于非常大的数据集，一个可行的方法是随机选择数据集的一个子集，对其进行网格搜索。然后在完整的数据集上进行仅有更好区域的网格搜索。**

## 4.附录（上述步骤的补充）

在某些情况下，经过以上建议的步骤还不够好，所以可能需要其他技术，如特征选择。
经验表明，该该程序对那些没有很多特征的数据来说效果很好。                                如果有大量特征属性，可能有必要在将数据交给SVM之前，需要选择其中的一个子集。

### A.建议程序的例子

在本附录中，我们将所提出的程序的准确性与一般初学者经常使用的程序进行比较。
实验是通过使用LIBSVM软件对表1中提到的三个问题进行的。![](file:///home/egotist/SVM/table1.jpg)
使用LIBSVM软件对表1中提到的三个问题进行实验。



对于每个问题，

首先列出了直接训练和测试的准确性。

其次，显示了有无缩放的准确性差异。(根据第2.2节中的讨论。训练集属性的范围必须被保存，以便我们能够在缩放测试集时恢复它们。)

第三，所提出的程序的准确性(缩放，然后选择模型)。

最后，展示了使用LIBSVM中的一个工具，它可以自动完成整个过程。类似下面介绍的grid.py这样的参数选择工具在 R-LIBSVM界面中也有类似的参数选择工具（见函数tune）。

#### A.1 Astroparticle

+ 具有默认参数的原始集

