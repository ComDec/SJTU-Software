# SVM实操

519111910306
姜戈

## 1.介绍
一个分类任务通常包括将数据分离成**训练集**和**测试集**。
训练集中的每个实例包含一个“目标值”(即类标签)和几个“属性”(即**特征或观察到的变量**)。SVM的目标是生成一个模型(基于训练数据)，该模型仅给出测试数据的属性来预测测试数据的目标值。

给定一个实例标签的训练集$(x_i,y_i),i=1,\dots,l; x_i\in R^n,y\in \{1,-1\}$

优化问题的形式![image-20210514192254114](C:\Users\chell\AppData\Roaming\Typora\typora-user-images\image-20210514192254114.png)

在这里，训练向量$x_i$被映射到一个更高维的空间利用函数$φ$。支持向量机找到一个边界最大的线性分离超平面

在这个高维空间中。C是误差项的惩罚参数

$K(x_i,x_j)≡φ(xi)^T φ(xj)$称为核函数。

以下是四个基本的核函数：

+ 线性：$K(X_i,X_j)=x_i ^Tx_j$
+ 多项式：$K(X_i,X_j)=(\gamma x_i ^Tx_j+r)^d,\gamma >0$
+ 径向基:$K(x_i , x_j ) = exp(−γ||x_i − x_j||^2 ), γ > 0.$
+ sigmoid:$K(x_i , x_j ) = tanh(γx_i^ T x_j + r)$

$γ、r、d$是核参数。

### 1.1实际例子

### 1.2过程设计

+ 将数据转换为SVM包的格式
+ 对数据进行简单的缩放
+ 考虑RBF核$K(x,y) = e^{−\gamma ||x−y||^2}$
+ 使用交叉验证找到最佳参数$C$和$γ$
+ 使用最佳参数$C$和$γ$训练整个训练集
+ 测试

接下来将在下面的部分中详细讨论这个过程

## 2.数据处理

### 2.1范畴特征

SVM需要将每个样本数据表示为实数组成的向量，因此对于分类的属性，需要将其转化为numeric data。推荐对m-category attribute表示为m个number，其中1个为1，剩余m-1个为0.

例如水果的颜色包括{红，绿，蓝}，那么采用categorical feature进行表示得到(0,0,1)(0,1,0)和(1,0,0). 我们的经验是如果属性值的数量没有特别大，那么比用单个数字表示是更稳定的。

最佳参数可能会受到数据集大小的影响，但在实践中，从交叉验证中得到的参数已经适合整个训练集。但在实践中，从交叉验证中得到的参数已经适用于整个训练集。

### 2.2缩放

在应用SVM之前进行缩放是非常重要的。缩放的主要优点是

1. 避免大数值范围内的属性支配小数值范围内的属性。

2. 避免计算过程中的数字困难。因为核值通常取决于特征向量的内积，例如线性核和多项式核，大的属性值可能会导致数值上的问题。

例如：

> 建议线性缩放每个属性到[-1; +1]或[0; 1]的范围。

当然，也必须用同样的方法来扩展训练和测试数据。
例如，假设我们将训练数据的第一个属性从[-10; +10缩放]到[-1; +1]。如果测试数据的第一个属性位于[-11; +8]的范围内，则必须将测试数据缩放到[-1.1；+0.8]。**(附录B)**

## 3.模型选择

尽管在第1节中只提到了四个常见的内核，但必须决定首先尝试哪一个。然后选择惩罚参数C和内核参数$\gamma$的选择

四个常见的kernel

1. linear: ![[公式]](https://www.zhihu.com/equation?tex=K%28x%2Cz%29%3Dx%5E%7BT%7Dz) 
2. polynomial: ![[公式]](https://www.zhihu.com/equation?tex=K%28x%2Cz%29%3D%28%5Cgamma+x%5E%7BT%7Dz%2B%5Cgamma%29%5E%7Bd%7D%2C%5Cgamma%3E0) 
3. radial basis function(RBF): ![[公式]](https://www.zhihu.com/equation?tex=K%28x%2Cz%29%3De%5E%7B-%5Cgamma+%7C%7Cx-z%7C%7C%5E%7B2%7D%7D) 
4. sigmoid kernel: ![[公式]](https://www.zhihu.com/equation?tex=K%28x%2Cz%29%3Dtanh%28%5Cgamma+x%5E%7BT%7Dz%2Br%29.)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%2Cr%2Cd) 是kernel的参数。

### 3.1 RBF核

一般来说，**RBF核是一个合理的第一选择**。这种内核**非线性**地将样本映射到一个更高的维度空间，所以它与线性核不同，可以处理当类标签和属性之间的关系是非线性的情况。
此外。线性核是RBF的一个特例 ，因为线性核有一个惩罚参数C。
因为带有惩罚参数C的线性核与带有某些参数（C，γ）的RBF核具有相同的性能。
一些参数(C, γ)。此外，在某些参数下，sigmoid核的表现与RBF类似。

第二个原因是超参数的数量，它影响了模型选择的复杂性。多项式核比RBF核有更多的超参数。

最后，RBF核有较少的数字困难。一个关键点是$0 <K_{ij} ≤ 1$，与此相反，多项式核的核值可能达到无穷大。此外，必须注意，某些参数下，sigmoid核是无效的（也就是说，并不是两个向量的内积）。

### 3.2 交叉验证和网格搜索（重点）

RBF核有两个参数。$C和γ$。对于哪个C和γ对给定的问题是最好的是未知的；因此，必须进行某种模型选择（参数搜索）。我们的目标是确定好的（C, γ），以便分类器能够准确地预测未知的数据。

分类器能够准确预测未知数据（即测试数据）。需要注意的是实现高的训练精度可能没有用（即一个分类器能够准确地预测的训练数据的类标签确实是已知的）。

**所以一个常见的策略是将数据集分成两部分，其中一部分被认为是未知。从 "未知 "集得到的预测准确率更准确地反映了对独立数据集进行分类的性能。这个程序的一个改进版本该程序的改进版本被称为交叉验证。**

在v-fold交叉验证法中，我们首先将训练集分成v个大小相等的子集。依次用剩余的v-1个子集上训练的分类器测试一个子集。因此，整个训练集的每个实例都被预测了一次，所以交叉验证的准确性是指正确分类的数据的百分比。

交叉验证程序可以防止过拟合问题,见下图例子![](file:///home/egotist/SVM/figure1.jpg)
                一个过拟合的分类器和一个更好的分类器（填充的圆圈和
                  三角形是训练数据，而空心圆和三角形是测试数据。）

图1a和1b中的分类器的测试精度并不高，因为它过度拟合了训练数据。
如果我们把图1a和1b中的训练和测试数据看作是交叉验证中的训练和验证集，那么准确率就不高了。
另一方面，图1c和1d中的分类器没有过度拟合训练数据，并给出了较好的交叉验证结果。训练和测试的准确度也更好。

所以建议使用交叉验证法对$C和γ$进行 "网格搜索"。对各种$(C, γ)$值进行尝试，并挑选出交叉验证准确率最高的一个。
我们发现，**尝试指数级增长的$C和γ$序列是一种实用的方法，可以识别出好的参数**（例如，$C = 2^-5 , 2^-3 , ... ... , 2……15 ,
γ = 2^-15 , 2^-13 , . . . , 2^3 $).网格搜索思路简单但过于浪费性能。

事实上，有几种的先进方法可以节省计算成本，例如，通过近似的交叉验证率。

然而，有两个动机使我们更倾向于采用简单的网格搜索法:

1. 其一是在心理上，不放心使用那些避免了用近似值或启发式方法来避免详尽的参数搜索。

2. 计算时间并没有比其他方法多很多因为只有两个参数，况且grid-search可以并行因为搜索的 ![[公式]](https://www.zhihu.com/equation?tex=C%2C%5Cgamma) 是独立的，其他方法往往是iterative process，例如沿着路径查找，这很难并行

由于做一个完整的网格搜索可能仍然很费时，我们建议首先使用泛化的网格。在确定了网格上一个 "较好 "的区域后，就可以对该区域进行更细的网格搜索。为了说明这一点，我们做了一个实验

   我们首先使用一个粗略的网格，发现最佳的（C, γ）是$（2^3 , 2^{-5} ）$。交叉验证率为77.5%。![](file:///home/egotist/SVM/figure2.jpg)接下来，我们在$（2^3 , 2^{-5} ）$附近进行了更细的网格搜索。
   (2 3 , 2 -5 )附近进行更细的网格搜索（图3），得到更好的交叉验证率77.6%。
   在(2 3.25 , 2 -5.25 )获得更好的交叉验证率。![](file:///home/egotist/SVM/figure3.jpg)在找到最佳的(C, γ)之后，整个训练集再次被训练,来生成最终的分类器。



上述方法对有数千或更多数据点的问题很有效。
 **对于非常大的数据集，一个可行的方法是随机选择数据集的一个子集，对其进行网格搜索。然后在完整的数据集上进行仅有更好区域的网格搜索。**

## 4.附录（上述步骤的补充）

在某些情况下，经过以上建议的步骤还不够好，所以可能需要其他技术，如特征选择。
经验表明，该该程序对那些没有很多特征的数据来说效果很好。                                如果有大量特征属性，可能有必要在将数据交给SVM之前，需要选择其中的一个子集。

### A.建议程序的例子

在本附录中，我们将所提出的程序的准确性与一般初学者经常使用的程序进行比较。
实验是通过使用LIBSVM软件对表1中提到的三个问题进行的。![](file:///home/egotist/SVM/table1.jpg)
使用LIBSVM软件对表1中提到的三个问题进行实验。



对于每个问题，

首先列出了直接训练和测试的准确性。

其次，显示了有无缩放的准确性差异。(根据第2.2节中的讨论。训练集属性的范围必须被保存，以便我们能够在缩放测试集时恢复它们。)

第三，所提出的程序的准确性(缩放，然后选择模型)。

最后，展示了使用LIBSVM中的一个工具，它可以自动完成整个过程。类似下面介绍的grid.py这样的参数选择工具在 R-LIBSVM界面中也有类似的参数选择工具（见函数tune）。

#### A.1 Astroparticle

+ 具有默认参数的原始集



## libSVM接口

### 训练数据格式

libsvm的训练数据格式如下：

```

<label> <index1>:<value1> <index2>:<value2> ...
```

示例：

```

1 1:2.927699e+01 2:1.072510e+02 3:1.149632e-01 4:1.077885e+02
```

### 主要类型

- `svm_problem`

保存定义SVM模型的训练数据

- `svm_parameter`

存储训练SVM模型所需的各种参数

- `svm_model`

完成训练的SVM模型

- `svm_node`

模型中一个特征的值，只包含一个整数索引和一个浮点值属性。

### 主要接口：

-`svm_problem(y, x)`

由训练数据y,x创建svm_problem对象

- `svm_train()`

svm_train有3个重载：

```

model = svm_train(y, x [, 'training_options'])
model = svm_train(prob [, 'training_options'])
model = svm_train(prob, param)
```

用于训练svm_model模型

- `svm_parameter(cmd)`

创建svm_parameter对象，参数为字符串。

示例：

```

param = svm_parameter('-t 0 -c 4 -b 1')
```

- `svm_predict()`

调用语法：

```

p_labs, p_acc, p_vals = svm_predict(y, x, model [,'predicting_options'])
```

参数：

`y` 测试数据的标签

`x` 测试数据的输入向量

`model`为训练好的SVM模型。

返回值：

`p_labs`是存储预测标签的列表。

`p_acc`存储了预测的精确度，均值和回归的平方相关系数。

`p_vals`在指定参数'-b 1'时将返回判定系数(判定的可靠程度)。

这个函数不仅是测试用的接口，也是应用状态下进行分类的接口。比较奇葩的是需要输入测试标签y才能进行预测，因为y不影响预测结果可以用0向量代替。

- `svm_read_problem`

读取LibSVM格式的训练数据：

```

y, x = svm_read_problem('data.txt')
```

- `svm_save_model`

将训练好的svm_model存储到文件中：

```

svm_save_model('model_file', model)
```

model_file的内容：

```

svm_type c_svc
kernel_type linear
nr_class 2
total_sv 2
rho 0
label 1 -1
probA 0.693147
probB 2.3919e-16
nr_sv 1 1
SV
0.25 1:1 2:1 
-0.25 1:-1 2:-1 
```

- `svm_load_model`

读取存储在文件中的svm_model:

```

 model = svm_load_model('model_file')
```

### 调整SVM参数

LibSVM在训练和预测过程中需要一系列参数来调整控制。

svm_train的参数：

- `-s` SVM的类型(svm_type)

  - 0 -- C-SVC(默认)

    使用惩罚因子(Cost)的处理噪声的多分类器

  - 1 -- nu-SVC(多分类器)

    按照错误样本比例处理噪声的多分类器

  - 2 -- one-class SVM

    一类支持向量机，可参见"SVDD"的相关内容

  - 3 -- epsilon-SVR(回归)

    epsilon支持向量回归

  - 4 -- nu-SVR(回归)

- `-t` 核函数类型(kernel_type)

  - 0 -- linear(线性核)

  - 1 -- polynomial(多项式核)

  - 2 -- radial basis function(RBF,径向基核/高斯核)

  - 3 -- sigmoid(S型核)

  - 4 -- precomputed kernel(预计算核)：

    核矩阵存储在`training_set_file`中

下面是调整SVM或核函数中参数的选项：

- `-d` 调整核函数的degree参数，默认为3
- `-g` 调整核函数的gamma参数，默认为`1/num_features`
- `-r` 调整核函数的coef0参数，默认为`0`
- `-c` 调整C-SVC, epsilon-SVR 和 nu-SVR中的Cost参数，默认为`1`
- `-n` 调整nu-SVC, one-class SVM 和 nu-SVR中的错误率nu参数，默认为`0.5`
- `-p` 调整epsilon-SVR的loss function中的epsilon参数，默认`0.1`
- `-m` 调整内缓冲区大小,以MB为单位，默认`100`
- `-e` 调整终止判据，默认`0.001`
- `-wi`调整C-SVC中第i个特征的Cost参数

调整算法功能的选项：

- `-b` 是否估算正确概率,取值0 - 1，默认为`0`
- `-h` 是否使用收缩启发式算法(shrinking heuristics),取值0 - 1，默认为`0`
- `-v` 交叉校验
- `-q` 静默模式

.



输出如下： 　　

#iter为迭代次数，nu 是你选择的核函数类型的参数，obj为SVM文件转换为的二次规划求解获得的最小值，rho为判决函数的偏置项b，nSV 为标准支持向量个数(0<a[i]<c)，nBSV为边界上的支持向量个数(a[i]=c)，Total nSV为支持向量总个数（对于两类来讲，由于只有一个分类模型Total nSV = nSV，可是对于多类，这个是各个分类模型的nSV之和）。